This document describes a conceptual design for the targeting and
scheduling system for SDSS-V.

Notes from discussion:
----------------------

* Tile -> design
* Need to build in flexibility to get some objects of a class?
* What cadence classes are most important? How do we quantify this?
* What is criterion for adding a design?
* Can we quantify value? 
* Queue observing for HET: weights, synoptic targets,
  etc. Supercommittee proposes different weights. Weight includes
  future opportunity.
* SPIE: queue-interested group -- Austin area

Sequence of events
------------------

1. Targeting

 a. Fundamental catalogs (Gaia, 2MASS, TIC, SDSS imaging, SDSS
 spectroscopy, others) are loaded into catalogdb. catalogdb has
 spatial indexing (q3c). 

 b. Fundamental catalogs are resolved into a single "catalog" table.
 All targets except targets-of-opportunity must exist in this table.
 Each entry receives a unique catalogid. The catalog stores the
 healpix pixel (nside=64, which is a bit less than 1 deg on a side)
 containing the RA/Dec.

 c. "target" is run. It receives input only from catalogdb.  It fills
 a table in targetdb which has: targeting bit mask, epochs, exposures
 per epoch, the ideal cadence per epoch (in days since first epoch),
 the softness in the cadence requirement per epoch (in units of
 days), and a name for the category of cadences. Skies and standards
 are included in this list of targets.

 d. The target table is output to disk as a set of FITS files
 organized by healpix pixel, for archival purposes. 

2. Tiling (call it a design)

 a. A set of fields are defined that cover the area with the
 appropriate density. This will almost certainly be a by-hand
 task.

 b. Teams define the cadence which they need for each field,
 driven by the targets it covers.

  * epochs:
	    number of epochs necessary for the field. 
  * exposures:
	    number of exposures in each epoch of the field.
  * ideal cadence:
	    for each epoch, ideal number of days since first epoch
  * cadence softness:
	    for each epoch, a quantification in days of how soft the
	    requirement is
  * cadence name:
	    name of cadence desired for the pixel

 Although ultimately the requirement is on the cadence and S/N of
 individual targets, in order to observe a target, you need to target
 the whole region around it, which is why these requirements come in
 at the field level.

 c. "tiling" is run. Note that the iterative sequence below is an
 attempt to cast this difficult global problem into a set of tractable
 ones. This is still R&D.

  i. The code defines a set of tiles (zero or more for each field),
     and determine which exposures of which targets are taken in each
     tile. Note that at this stage there is no cadence requirement
     imposed. This can be done with a network flow as done for the
		 plates. It establishes a starting point for the observations that
		 at least observes everything. Skies and standards are included.

  ii. Given the set of tiles, tiling defines a sequence of
      observations based on the cadence criteria for each field and
      the LST opportunity of each field. How to approach this is
      unclear, but constraint programming techniques may work.

  iii. Each tile now has specific planned sequence. In this context,
       we now assign targets to the tiles applying cadence criteria. 
       Again constraint programming may work here, but how to solve
			 this is unclear. Skies and standards are included.

  iv. There may be some iteration in the above steps.

  v. For each tile, we need to define some window of opportunity for
     its observation in the plan.

3. Training the strategy:

 a. Define a global metric for assessing success. This metric should
 be based on the outcomes for the targets (not the fields). 

 b. Define a strategy for selecting the next tile based on its
 observability and its window of opportunity. The strategy may allow a
 periodic rerunning of tiling based on achieved results.  The strategy
 may have tunable parameters. 

 c. Run simulations under that strategy.

 d. Iterate on the strategy parameters to achieve best results on
 global metric.

 e. Assess strategy based on individual program outcomes.

 f. Iterate on choices of (a) and (b) to best achieve outcomes of (e),
 whatever that means.

4. Survey operations

 a. Prior to the survey, the targets and tiles are prepared. This means
    designing and running targeting software across the whole sky, and 
    designing a full set of tiles.

 b. Each night, the tiles and fiber plan is used throughout the night
    to pick the next tile to observe and its duration or S/N
    requirement for that night. At this point targets-of-opportunity  
    may be added. The configuration used for each observation is stored 
    and the results of each exposure is stored. Once the duration or 
    S/N requirement is hit for a tile it is not reobserved that night.

 c. Each morning, the fpsdb database is updated with the results of
    the night. The status of each tile and overall fiber plan for the
    tiles may be updated as well. This retile may mean, for example, that
    individual fibers that performed either better or worse than
    expected may be reassigned in the future-looking fiber plan
    (expressed in the "fiber" table of fpsdb). 

fpsdb should be designed to be lightweight. It would be instantiated
at Utah, LCO, and APO. Targets would be designated for one or other or
both. A daytime process would need to update the Utah version based on
LCO and based on APO, perform retiling, and push back the full updated
database to the observatories. If connectivity was lost, the retiling
operation would be skipped that day and updates would happen later.

Software products
-----------------

Right now everything is being developed in observesim. However,
ultimately we should be refactoring into separate products:

* catalogdb: to create and access catalogdb

* targetdb: to create and access targetdb

* fpsdb: to create and access fpsdb

* target: to run targeting

* tile: to run tiling

* scheduler: software to run scheduler

* configure: software to assign and test fiber configurations

* observesim: software to run simulations of above

Databases
---------

* catalogdb: input catalog database (heavy-weight)

* targetdb: target catalog database (medium-weight)

* fpsdb: operational database (light-weight)

Glossary
--------

* Catalogs: refers to the fundamental catalogs from which the list of
  targets is derived (e.g. Gaia, 2MASS, TESS Input Catalog, etc.).

* Targeting: refers to the process of identifying a catalog object as
  a potential target for spectroscopy. Results in a global list of
  these targets in the database. This does not guarantee a fiber is
  assigned.

* Target: An astronomical object that has been identified in targeting
  to potentially receive a fiber.

* Field: refers to an RA/Dec center associated with a pointing of one
  of the telescopes.

* Tile: refers to an observation of the field, which may consist of
  one or more configurations, observed back-to-back.

* Configuration: refers to the conditions of an actual exposure of
  a field, and the specific assignments of fibers to targets.

Standards
---------

 * All target coordinates are given in columns named "ra", "dec",
   which are 64-bit precision and given in J2000 deg, with an "epoch"
   specified in decimal years, and with proper motions called "pmra"
   and "pmdec" in mas / year.

 * All field or tile center coordinates are given in columns  named
   "racen", "deccen", which are 64-bit precision and given in J2000
   deg.
 
